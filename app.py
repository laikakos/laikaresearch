import streamlit as st
import pandas as pd
from utils.text_processor import (
    extract_text_from_docx,
    extract_text_from_pdf,
    split_into_sentences,
    find_keyword_contexts,
    clean_text
)
from utils.models import analyze_text_with_all_models
from utils.visualizer import (
    create_emotion_radar_chart,
    create_results_dataframe
)

# Sayfa ayarlarƒ±
st.set_page_config(
    page_title="Qatar Sentiment Analysis",
    page_icon="üèÜ",
    layout="wide"
)

# Ba≈ülƒ±k
st.title("üèÜ Qatar D√ºnya Kupasƒ± Duygu Analizi")
st.markdown("### Almanca Haber Metinleri i√ßin √áoklu Model Kar≈üƒ±la≈ütƒ±rmasƒ±")

st.markdown("""
Bu uygulama 3 farklƒ± model ile Almanca metinlerde duygu analizi yapar:
- **Model 1:** Hƒ±zlƒ± Pilot (Guhr et al. LREC 2020)
- **Model 2:** Haber Metinleri (mdraw - √∂zelle≈ütirilmi≈ü)
- **Model 3:** Detaylƒ± Akademik (GoEmotions - 27 duygu)
""")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Ayarlar")
    
    # Anahtar kelimeler
    st.subheader("Anahtar Kelimeler")
    keywords_input = st.text_area(
        "Her satƒ±ra bir kelime",
        value="Qatar\nKatar\nWeltmeisterschaft\nFu√üball\nWM",
        height=150
    )
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    # Context window
    st.subheader("Context Window")
    context_before = st.slider("√ñnceki c√ºmle sayƒ±sƒ±", 0, 5, 3)
    context_after = st.slider("Sonraki c√ºmle sayƒ±sƒ±", 0, 5, 3)
    
    # Batch processing ayarƒ±
    st.subheader("Toplu ƒ∞≈üleme")
    batch_size = st.number_input("Batch boyutu", min_value=10, max_value=500, value=100, step=10)
    st.info(f"Her {batch_size} dosya i√ßin ilerleme g√∂sterilecek")
    
    st.markdown("---")
    st.markdown("**Geli≈ütirici:** laikaresearch")

# Ana i√ßerik
tab1, tab2, tab3 = st.tabs(["üìÑ Dosya Y√ºkle", "üìä Sonu√ßlar", "‚ÑπÔ∏è Hakkƒ±nda"])

with tab1:
    st.header("Dosya Y√ºkleme")
    
    uploaded_files = st.file_uploader(
        "Almanca haber dosyalarƒ±nƒ±zƒ± y√ºkleyin (.txt, .docx veya .pdf)",
        type=['txt', 'docx', 'pdf'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.warning(f"‚ö†Ô∏è {len(uploaded_files)} dosya y√ºklendi. B√ºy√ºk dosya sayƒ±sƒ± i√ßin i≈ülem uzun s√ºrebilir.")
        
        # T√ºm dosyalarƒ± birle≈ütir
        all_texts = []
        
        # Dosya y√ºkleme progress bar
        file_progress = st.progress(0)
        file_status = st.empty()
        
        failed_files = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                file_status.text(f"Dosya okunuyor: {uploaded_file.name} ({file_idx+1}/{len(uploaded_files)})")
                
                # Dosya tipine g√∂re okuma
                if uploaded_file.name.endswith('.docx'):
                    text = extract_text_from_docx(uploaded_file)
                elif uploaded_file.name.endswith('.pdf'):
                    text = extract_text_from_pdf(uploaded_file)
                elif uploaded_file.name.endswith('.txt'):
                    # TXT dosyalarƒ± i√ßin encoding denemesi
                    try:
                        text = uploaded_file.read().decode('utf-8')
                    except UnicodeDecodeError:
                        # UTF-8 ba≈üarƒ±sƒ±z olursa latin-1 dene
                        uploaded_file.seek(0)  # Dosya pozisyonunu ba≈üa al
                        text = uploaded_file.read().decode('latin-1')
                else:
                    st.warning(f"‚ö†Ô∏è Desteklenmeyen dosya tipi: {uploaded_file.name}")
                    continue
                
                # Temizlenmi≈ü metni kaydet
                cleaned_text = clean_text(text)
                
                if len(cleaned_text.strip()) > 0:
                    all_texts.append({
                        'filename': uploaded_file.name,
                        'text': cleaned_text
                    })
                else:
                    failed_files.append((uploaded_file.name, "Bo≈ü dosya"))
                
            except Exception as e:
                failed_files.append((uploaded_file.name, str(e)))
                st.error(f"‚ùå Hata: {uploaded_file.name} - {str(e)[:100]}")
            
            # Progress g√ºncelle
            file_progress.progress((file_idx + 1) / len(uploaded_files))
        
        file_status.empty()
        file_progress.empty()
        
        # Sonu√ß √∂zeti
        if all_texts:
            st.success(f"‚úÖ {len(all_texts)} dosya ba≈üarƒ±yla y√ºklendi")
        
        if failed_files:
            st.error(f"‚ùå {len(failed_files)} dosya y√ºklenemedi")
            with st.expander("Ba≈üarƒ±sƒ±z Dosyalar"):
                for fname, error in failed_files:
                    st.write(f"- **{fname}**: {error}")
        
        if not all_texts:
            st.error("‚ùå Hi√ßbir dosya ba≈üarƒ±yla y√ºklenemedi!")
        else:
            # Toplam istatistikler
            total_chars = sum(len(t['text']) for t in all_texts)
            st.info(f"üìù Toplam karakter: {total_chars:,}")
            
            # Dosya listesi (ilk 20 dosya)
            with st.expander(f"üìÇ Y√ºklenen Dosyalar (ƒ∞lk 20/{len(all_texts)})"):
                for i, item in enumerate(all_texts[:20]):
                    st.write(f"{i+1}. **{item['filename']}** - {len(item['text']):,} karakter")
                if len(all_texts) > 20:
                    st.write(f"... ve {len(all_texts) - 20} dosya daha")
            
            # √ñnizleme
            with st.expander("üìÑ ƒ∞lk Dosya √ñnizleme"):
                preview_text = all_texts[0]['text']
                st.text(preview_text[:1000] + "..." if len(preview_text) > 1000 else preview_text)
            
            # Analiz butonu
            if st.button("üöÄ Analizi Ba≈ülat", type="primary"):
                with st.spinner("Analiz yapƒ±lƒ±yor..."):
                    
                    all_results = []
                    
                    # Genel progress bar
                    overall_progress = st.progress(0)
                    overall_status = st.empty()
                    
                    # Her dosya i√ßin analiz
                    for file_idx, item in enumerate(all_texts):
                        overall_status.text(f"üìÑ Analiz ediliyor: {item['filename']} ({file_idx+1}/{len(all_texts)})")
                        
                        try:
                            # C√ºmlelere ayƒ±r
                            sentences = split_into_sentences(item['text'])
                            
                            # Anahtar kelime e≈üle≈ümelerini bul
                            matches = find_keyword_contexts(
                                sentences, 
                                keywords,
                                context_before,
                                context_after
                            )
                            
                            if matches:
                                # Her e≈üle≈üme i√ßin analiz yap
                                for idx, match in enumerate(matches):
                                    # Analiz
                                    analysis = analyze_text_with_all_models(match['context'])
                                    
                                    all_results.append({
                                        'filename': item['filename'],
                                        **match,
                                        **analysis
                                    })
                            
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Analiz hatasƒ±: {item['filename']} - {str(e)[:100]}")
                        
                        # Overall progress g√ºncelle
                        overall_progress.progress((file_idx + 1) / len(all_texts))
                    
                    overall_status.empty()
                    overall_progress.empty()
                    
                    if all_results:
                        # Sonu√ßlarƒ± session state'e kaydet
                        st.session_state['results'] = all_results
                        st.session_state['analyzed'] = True
                        
                        st.success(f"‚úÖ Analiz tamamlandƒ±! Toplam {len(all_results)} e≈üle≈üme bulundu. 'Sonu√ßlar' sekmesine gidin.")
                    else:
                        st.error("‚ùå Hi√ßbir dosyada anahtar kelime bulunamadƒ±!")

with tab2:
    st.header("üìä Analiz Sonu√ßlarƒ±")
    
    if 'analyzed' in st.session_state and st.session_state['analyzed']:
        results = st.session_state['results']
        
        st.info(f"üìà Toplam {len(results)} baƒülam analiz edildi")
        
        # Filtreleme se√ßenekleri
        with st.expander("üîç Filtreleme Se√ßenekleri"):
            col1, col2 = st.columns(2)
            
            with col1:
                # Dosyaya g√∂re filtrele
                all_files = sorted(list(set([r.get('filename', 'N/A') for r in results])))
                selected_files = st.multiselect(
                    "Dosya Se√ß",
                    options=all_files,
                    default=all_files[:5] if len(all_files) > 5 else all_files
                )
            
            with col2:
                # Anahtar kelimeye g√∂re filtre
                all_keywords = sorted(list(set([r['keyword'] for r in results])))
                selected_keywords = st.multiselect(
                    "Anahtar Kelime Se√ß",
                    options=all_keywords,
                    default=all_keywords
                )
        
        # Filtrelenmi≈ü sonu√ßlar
        filtered_results = [
            r for r in results 
            if r.get('filename', 'N/A') in selected_files and r['keyword'] in selected_keywords
        ]
        
        st.info(f"üîé G√∂sterilen: {len(filtered_results)} / {len(results)}")
        
        # Her sonucu g√∂ster
        for idx, result in enumerate(filtered_results):
            with st.expander(f"üîç {result.get('filename', 'N/A')} - E≈üle≈üme {idx+1}: '{result['keyword']}' - C√ºmle {result['sentence_index']}"):
                
                # Context g√∂ster
                st.markdown("**üìù Baƒülam:**")
                st.write(result['context'])
                
                st.markdown("---")
                
                # Model sonu√ßlarƒ±
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("**Model 1: Hƒ±zlƒ± Pilot**")
                    sentiment_1 = result['model_1']['sentiment']
                    st.metric("Sentiment", sentiment_1)
                
                with col2:
                    st.markdown("**Model 2: Haber**")
                    sentiment_2 = result['model_2']['sentiment']
                    st.metric("Sentiment", sentiment_2)
                
                with col3:
                    st.markdown("**Model 3: Detaylƒ±**")
                    top_emotion = result['model_3']['top_emotions'][0]
                    st.metric(
                        "Top Duygu", 
                        top_emotion['label'],
                        f"{top_emotion['score']:.2%}"
                    )
                
                # Model 3 detaylarƒ±
                st.markdown("**üé≠ Top 5 Duygu (Model 3):**")
                emotion_df = pd.DataFrame(result['model_3']['top_emotions'])
                st.dataframe(emotion_df, use_container_width=True)
                
                # Radar chart
                fig = create_emotion_radar_chart(result['model_3']['top_emotions'])
                if fig:
                    st.plotly_chart(fig, use_container_width=True)
        
        # √ñzet tablo
        st.markdown("---")
        st.subheader("üìã √ñzet Tablo")
        df = create_results_dataframe(results)
        st.dataframe(df, use_container_width=True)
        
        # CSV indirme
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• Sonu√ßlarƒ± CSV olarak indir",
            data=csv,
            file_name="qatar_sentiment_results.csv",
            mime="text/csv"
        )
        
    else:
        st.info("üëà √ñnce 'Dosya Y√ºkle' sekmesinden dosya y√ºkleyin ve analiz ba≈ülatƒ±n.")

with tab3:
    st.header("‚ÑπÔ∏è Proje Hakkƒ±nda")
    
    st.markdown("""
    ### üéØ Ama√ß
    Qatar D√ºnya Kupasƒ± ile ilgili Almanca haber metinlerinde duygu analizi yapmak ve 
    farklƒ± modellerin performanslarƒ±nƒ± kar≈üƒ±la≈ütƒ±rmak.
    
    ### üìö Kullanƒ±lan Modeller
    
    **1. Model 1: Hƒ±zlƒ± Pilot**
    - Oliver Guhr et al. (LREC 2020)
    - 1.8M Almanca √∂rnek
    - 3 kategori: positive, negative, neutral
    - F1 Score: ~0.96
    
    **2. Model 2: Haber Metinleri**
    - mdraw/german-news-sentiment-bert
    - Haber dili i√ßin √∂zelle≈ütirilmi≈ü
    - 2007-2019 g√∂√ß haberleri ile eƒüitilmi≈ü
    
    **3. Model 3: Detaylƒ± Akademik**
    - GoEmotions (Google, ACL 2020)
    - 27 farklƒ± duygu kategorisi
    - √áok dilli BERT
    
    ### üî¨ Metodoloji
    - **Context Window:** Anahtar kelimenin 3 c√ºmle √∂ncesi ve sonrasƒ±
    - **√áoklu Model:** √ú√ß farklƒ± yakla≈üƒ±mƒ±n kar≈üƒ±la≈ütƒ±rmasƒ±
    - **Akademik Standart:** Peer-reviewed modeller
    
    ### üìñ Kaynaklar
    - [Guhr et al. 2020 - LREC](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)
    - [GoEmotions - ACL 2020](https://aclanthology.org/2020.acl-main.372/)
    - [GitHub Repository](https://github.com/laikakos/laikaresearch)
    
    ### ‚ö° Performans ƒ∞pu√ßlarƒ± (2500 PDF i√ßin)
    - Dosyalar batch olarak i≈ülenir
    - Her 100 dosyada ilerleme g√∂sterilir
    - Hatalƒ± dosyalar atlanƒ±r ve listelenir
    - Toplam s√ºre: ~30-60 dakika (dosya boyutuna baƒülƒ±)
    """)

st.markdown("---")
st.markdown("üí° **ƒ∞pucu:** Sidebar'dan anahtar kelimeleri ve context window ayarlarƒ±nƒ± √∂zelle≈ütirebilirsiniz.")
