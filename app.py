import streamlit as st
import pandas as pd
from utils.text_processor import (
    extract_text_from_docx,
    extract_text_from_pdf,
    split_into_sentences,
    find_keyword_contexts,
    clean_text
)
from utils.models import analyze_text_with_all_models
from utils.visualizer import (
    create_emotion_radar_chart,
    create_results_dataframe,
    create_sentiment_distribution_chart,
    create_file_summary_chart,
    create_keyword_summary_chart
)
import os

# Sayfa ayarlarÄ±
st.set_page_config(
    page_title="Qatar Sentiment Analysis",
    page_icon="ğŸ†",
    layout="wide"
)

# BaÅŸlÄ±k
st.title("ğŸ† Qatar DÃ¼nya KupasÄ± Duygu Analizi")
st.markdown("### Almanca Haber Metinleri iÃ§in Ã‡oklu Model KarÅŸÄ±laÅŸtÄ±rmasÄ±")

st.markdown("""
Bu uygulama 3 farklÄ± model ile Almanca metinlerde duygu analizi yapar:
- **Model 1:** HÄ±zlÄ± Pilot (Guhr et al. LREC 2020)
- **Model 2:** Haber Metinleri (mdraw - Ã¶zelleÅŸtirilmiÅŸ)
- **Model 3:** DetaylÄ± Akademik (GoEmotions - 27 duygu)
""")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    
    # Anahtar kelimeler
    st.subheader("Anahtar Kelimeler")
    keywords_input = st.text_area(
        "Her satÄ±ra bir kelime",
        value="Qatar\nKatar\nWeltmeisterschaft\nFuÃŸball\nWM",
        height=150
    )
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    # Context window
    st.subheader("Context Window")
    context_before = st.slider("Ã–nceki cÃ¼mle sayÄ±sÄ±", 0, 5, 3)
    context_after = st.slider("Sonraki cÃ¼mle sayÄ±sÄ±", 0, 5, 3)
    
    # Batch processing ayarÄ±
    st.subheader("Toplu Ä°ÅŸleme")
    batch_size = st.number_input("Batch boyutu", min_value=10, max_value=500, value=100, step=10)
    st.info(f"Her {batch_size} dosya iÃ§in ilerleme gÃ¶sterilecek")
    
    st.markdown("---")
    st.markdown("**GeliÅŸtirici:** laikaresearch")

# Dosya tipini kontrol eden yardÄ±mcÄ± fonksiyon
def get_file_extension(filename):
    """Dosya uzantÄ±sÄ±nÄ± gÃ¼venli ÅŸekilde al"""
    if '.' in filename:
        return filename.rsplit('.', 1)[-1].lower()
    return ''

# Ana iÃ§erik
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ Dosya YÃ¼kle", "ğŸ“Š SonuÃ§lar", "ğŸ“ˆ Ä°statistikler", "â„¹ï¸ HakkÄ±nda"])

with tab1:
    st.header("Dosya YÃ¼kleme")
    
    uploaded_files = st.file_uploader(
        "Almanca haber dosyalarÄ±nÄ±zÄ± yÃ¼kleyin (.txt, .docx veya .pdf)",
        type=['txt', 'docx', 'pdf'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.warning(f"âš ï¸ {len(uploaded_files)} dosya yÃ¼klendi. BÃ¼yÃ¼k dosya sayÄ±sÄ± iÃ§in iÅŸlem uzun sÃ¼rebilir.")
        
        # TÃ¼m dosyalarÄ± birleÅŸtir
        all_texts = []
        
        # Dosya yÃ¼kleme progress bar
        file_progress = st.progress(0)
        file_status = st.empty()
        
        failed_files = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                file_status.text(f"Dosya okunuyor: {uploaded_file.name} ({file_idx+1}/{len(uploaded_files)})")
                
                # Dosya uzantÄ±sÄ±nÄ± gÃ¼venli ÅŸekilde al
                file_extension = get_file_extension(uploaded_file.name)
                
                # Dosya tipine gÃ¶re okuma
                if file_extension == 'docx':
                    text = extract_text_from_docx(uploaded_file)
                elif file_extension == 'pdf':
                    text = extract_text_from_pdf(uploaded_file)
                elif file_extension == 'txt':
                    # TXT dosyalarÄ± iÃ§in encoding denemesi
                    try:
                        text = uploaded_file.read().decode('utf-8')
                    except UnicodeDecodeError:
                        # UTF-8 baÅŸarÄ±sÄ±z olursa latin-1 dene
                        uploaded_file.seek(0)
                        try:
                            text = uploaded_file.read().decode('latin-1')
                        except:
                            uploaded_file.seek(0)
                            text = uploaded_file.read().decode('cp1252')
                else:
                    failed_files.append((uploaded_file.name, f"Desteklenmeyen dosya tipi: .{file_extension}"))
                    continue
                
                # TemizlenmiÅŸ metni kaydet
                cleaned_text = clean_text(text)
                
                if len(cleaned_text.strip()) > 0:
                    all_texts.append({
                        'filename': uploaded_file.name,
                        'text': cleaned_text
                    })
                else:
                    failed_files.append((uploaded_file.name, "BoÅŸ dosya"))
                
            except Exception as e:
                failed_files.append((uploaded_file.name, str(e)))
                st.error(f"âŒ Hata: {uploaded_file.name} - {str(e)[:100]}")
            
            # Progress gÃ¼ncelle
            file_progress.progress((file_idx + 1) / len(uploaded_files))
        
        file_status.empty()
        file_progress.empty()
        
        # SonuÃ§ Ã¶zeti
        if all_texts:
            st.success(f"âœ… {len(all_texts)} dosya baÅŸarÄ±yla yÃ¼klendi")
        
        if failed_files:
            st.error(f"âŒ {len(failed_files)} dosya yÃ¼klenemedi")
            with st.expander("BaÅŸarÄ±sÄ±z Dosyalar"):
                for fname, error in failed_files:
                    st.write(f"- **{fname}**: {error}")
        
        if not all_texts:
            st.error("âŒ HiÃ§bir dosya baÅŸarÄ±yla yÃ¼klenemedi!")
        else:
            # Toplam istatistikler
            total_chars = sum(len(t['text']) for t in all_texts)
            st.info(f"ğŸ“ Toplam karakter: {total_chars:,}")
            
            # Dosya listesi (ilk 20 dosya)
            with st.expander(f"ğŸ“‚ YÃ¼klenen Dosyalar (Ä°lk 20/{len(all_texts)})"):
                for i, item in enumerate(all_texts[:20]):
                    st.write(f"{i+1}. **{item['filename']}** - {len(item['text']):,} karakter")
                if len(all_texts) > 20:
                    st.write(f"... ve {len(all_texts) - 20} dosya daha")
            
            # Ã–nizleme
            with st.expander("ğŸ“„ Ä°lk Dosya Ã–nizleme"):
                preview_text = all_texts[0]['text']
                st.text(preview_text[:1000] + "..." if len(preview_text) > 1000 else preview_text)
            
            # Analiz butonu
            if st.button("ğŸš€ Analizi BaÅŸlat", type="primary"):
                with st.spinner("Analiz yapÄ±lÄ±yor..."):
                    
                    all_results = []
                    
                    # Genel progress bar
                    overall_progress = st.progress(0)
                    overall_status = st.empty()
                    
                    # YENÄ°: Ä°lerleme detay metrikleri
                    progress_cols = st.columns(4)
                    metric_files = progress_cols[0].empty()
                    metric_matches = progress_cols[1].empty()
                    metric_analyzed = progress_cols[2].empty()
                    metric_time = progress_cols[3].empty()
                    
                    import time
                    start_time = time.time()
                    
                    # Her dosya iÃ§in analiz
                    for file_idx, item in enumerate(all_texts):
                        # YENÄ°: Daha detaylÄ± ilerleme bilgisi
                        elapsed_time = time.time() - start_time
                        avg_time_per_file = elapsed_time / (file_idx + 1) if file_idx > 0 else 0
                        remaining_files = len(all_texts) - file_idx - 1
                        estimated_remaining = avg_time_per_file * remaining_files
                        
                        overall_status.text(
                            f"ğŸ“„ Analiz ediliyor: {item['filename']} "
                            f"({file_idx+1}/{len(all_texts)}) - "
                            f"Toplam eÅŸleÅŸme: {len(all_results)}"
                        )
                        
                        # Metrikler gÃ¼ncelle
                        metric_files.metric("Ä°ÅŸlenen Dosya", f"{file_idx+1}/{len(all_texts)}")
                        metric_matches.metric("Bulunan EÅŸleÅŸme", len(all_results))
                        metric_analyzed.metric("Analiz Edilen", len(all_results))
                        if estimated_remaining > 0:
                            mins, secs = divmod(int(estimated_remaining), 60)
                            metric_time.metric("Tahmini Kalan", f"{mins}d {secs}s")
                        
                        try:
                            # CÃ¼mlelere ayÄ±r
                            sentences = split_into_sentences(item['text'])
                            
                            # Anahtar kelime eÅŸleÅŸmelerini bul
                            matches = find_keyword_contexts(
                                sentences, 
                                keywords,
                                context_before,
                                context_after
                            )
                            
                            if matches:
                                # Her eÅŸleÅŸme iÃ§in analiz yap
                                for idx, match in enumerate(matches):
                                    # Analiz
                                    analysis = analyze_text_with_all_models(match['context'])
                                    
                                    all_results.append({
                                        'filename': item['filename'],
                                        **match,
                                        **analysis
                                    })
                                    
                                    # Her 10 eÅŸleÅŸmede metrikleri gÃ¼ncelle
                                    if idx % 10 == 0:
                                        metric_matches.metric("Bulunan EÅŸleÅŸme", len(all_results))
                                        metric_analyzed.metric("Analiz Edilen", len(all_results))
                            
                        except Exception as e:
                            st.warning(f"âš ï¸ Analiz hatasÄ±: {item['filename']} - {str(e)[:100]}")
                        
                        # Overall progress gÃ¼ncelle
                        overall_progress.progress((file_idx + 1) / len(all_texts))
                    
                    # Temizlik
                    overall_status.empty()
                    overall_progress.empty()
                    for col in progress_cols:
                        col.empty()
                    
                    # Toplam sÃ¼re
                    total_time = time.time() - start_time
                    mins, secs = divmod(int(total_time), 60)
                    
                    if all_results:
                        # SonuÃ§larÄ± session state'e kaydet
                        st.session_state['results'] = all_results
                        st.session_state['analyzed'] = True
                        
                        st.success(
                            f"âœ… Analiz tamamlandÄ±! "
                            f"Toplam {len(all_results)} eÅŸleÅŸme bulundu. "
                            f"SÃ¼re: {mins} dakika {secs} saniye. "
                            f"'SonuÃ§lar' sekmesine gidin."
                        )
                    else:
                        st.error("âŒ HiÃ§bir dosyada anahtar kelime bulunamadÄ±!")

with tab2:
    st.header("ğŸ“Š DetaylÄ± SonuÃ§lar")
    
    if 'analyzed' in st.session_state and st.session_state['analyzed']:
        results = st.session_state['results']
        
        st.info(f"ğŸ“ˆ Toplam {len(results)} baÄŸlam analiz edildi")
        
        # Filtreleme seÃ§enekleri
        with st.expander("ğŸ” Filtreleme SeÃ§enekleri"):
            col1, col2 = st.columns(2)
            
            with col1:
                # Dosyaya gÃ¶re filtrele
                all_files = sorted(list(set([r.get('filename', 'N/A') for r in results])))
                selected_files = st.multiselect(
                    "Dosya SeÃ§",
                    options=all_files,
                    default=all_files[:5] if len(all_files) > 5 else all_files
                )
            
            with col2:
                # Anahtar kelimeye gÃ¶re filtre
                all_keywords = sorted(list(set([r['keyword'] for r in results])))
                selected_keywords = st.multiselect(
                    "Anahtar Kelime SeÃ§",
                    options=all_keywords,
                    default=all_keywords
                )
        
        # FiltrelenmiÅŸ sonuÃ§lar
        filtered_results = [
            r for r in results 
            if r.get('filename', 'N/A') in selected_files and r['keyword'] in selected_keywords
        ]
        
        st.info(f"ğŸ” GÃ¶sterilen: {len(filtered_results)} / {len(results)}")
        
        # Her sonucu gÃ¶ster
        for idx, result in enumerate(filtered_results):
            with st.expander(f"ğŸ” {result.get('filename', 'N/A')} - EÅŸleÅŸme {idx+1}: '{result['keyword']}' - CÃ¼mle {result['sentence_index']}"):
                
                # Context gÃ¶ster
                st.markdown("**ğŸ“ BaÄŸlam:**")
                st.write(result['context'])
                
                st.markdown("---")
                
                # Model sonuÃ§larÄ±
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("**Model 1: HÄ±zlÄ± Pilot**")
                    sentiment_1 = result['model_1']['sentiment']
                    st.metric("Sentiment", sentiment_1)
                
                with col2:
                    st.markdown("**Model 2: Haber**")
                    sentiment_2 = result['model_2']['sentiment']
                    st.metric("Sentiment", sentiment_2)
                
                with col3:
                    st.markdown("**Model 3: DetaylÄ±**")
                    top_emotion = result['model_3']['top_emotions'][0]
                    st.metric(
                        "Top Duygu", 
                        top_emotion['label'],
                        f"{top_emotion['score']:.2%}"
                    )
                
                # Model 3 detaylarÄ±
                st.markdown("**ğŸ­ Top 5 Duygu (Model 3):**")
                emotion_df = pd.DataFrame(result['model_3']['top_emotions'])
                st.dataframe(emotion_df, use_container_width=True)
                
                # Radar chart
                fig = create_emotion_radar_chart(result['model_3']['top_emotions'])
                if fig:
                    st.plotly_chart(fig, use_container_width=True, key=f"radar_chart_{idx}")
        
        # Ã–zet tablo
        st.markdown("---")
        st.subheader("ğŸ“‹ Ã–zet Tablo")
        df = create_results_dataframe(results)
        st.dataframe(df, use_container_width=True)
        
        # CSV indirme
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ SonuÃ§larÄ± CSV olarak indir",
            data=csv,
            file_name="qatar_sentiment_results.csv",
            mime="text/csv"
        )
        
    else:
        st.info("ğŸ‘ˆ Ã–nce 'Dosya YÃ¼kle' sekmesinden dosya yÃ¼kleyin ve analiz baÅŸlatÄ±n.")

with tab3:
    st.header("ğŸ“ˆ Genel Ä°statistikler ve GÃ¶rselleÅŸtirmeler")
    
    if 'analyzed' in st.session_state and st.session_state['analyzed']:
        results = st.session_state['results']
        
        # Ã–zet metrikler
        st.subheader("ğŸ“Š Ã–zet Metrikler")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Toplam EÅŸleÅŸme", len(results))
        
        with col2:
            unique_files = len(set([r.get('filename', 'N/A') for r in results]))
            st.metric("Analiz Edilen Dosya", unique_files)
        
        with col3:
            unique_keywords = len(set([r['keyword'] for r in results]))
            st.metric("Bulunan Anahtar Kelime", unique_keywords)
        
        with col4:
            avg_per_file = len(results) / unique_files if unique_files > 0 else 0
            st.metric("Dosya BaÅŸÄ±na Ort. EÅŸleÅŸme", f"{avg_per_file:.1f}")
        
        st.markdown("---")
        
        # YENÄ°: OVERLAP ANALÄ°ZÄ°
        st.subheader("ğŸ”„ Overlap (Ãœst Ãœste Binme) Analizi")
        
        st.info("ğŸ’¡ Bu analiz, aynÄ± cÃ¼mlelerin farklÄ± context'lerde kaÃ§ kez analiz edildiÄŸini gÃ¶sterir.")
        
        # Her dosya iÃ§in cÃ¼mle kullanÄ±mlarÄ±nÄ± hesapla
        file_sentence_usage = {}
        
        for r in results:
            fname = r.get('filename', 'N/A')
            context_range = r.get('context_range', (0, 0))
            
            if fname not in file_sentence_usage:
                file_sentence_usage[fname] = []
            
            # Bu eÅŸleÅŸmede kullanÄ±lan tÃ¼m cÃ¼mle indekslerini ekle
            for sentence_idx in range(context_range[0], context_range[1]):
                file_sentence_usage[fname].append(sentence_idx)
        
        # Genel overlap istatistikleri
        all_used_sentences = []
        for sentences in file_sentence_usage.values():
            all_used_sentences.extend(sentences)
        
        total_sentence_usages = len(all_used_sentences)
        unique_sentences_used = len(set(all_used_sentences))
        
        if total_sentence_usages > 0:
            overlap_ratio = ((total_sentence_usages - unique_sentences_used) / total_sentence_usages) * 100
            avg_usage_per_sentence = total_sentence_usages / unique_sentences_used if unique_sentences_used > 0 else 0
        else:
            overlap_ratio = 0
            avg_usage_per_sentence = 0
        
        # Overlap metrikleri
        overlap_col1, overlap_col2, overlap_col3, overlap_col4 = st.columns(4)
        
        with overlap_col1:
            st.metric(
                "Toplam CÃ¼mle KullanÄ±mÄ±", 
                f"{total_sentence_usages:,}",
                help="TÃ¼m analizlerde kullanÄ±lan toplam cÃ¼mle sayÄ±sÄ± (tekrarlarla birlikte)"
            )
        
        with overlap_col2:
            st.metric(
                "Benzersiz CÃ¼mle", 
                f"{unique_sentences_used:,}",
                help="KaÃ§ farklÄ± cÃ¼mle analiz edildi"
            )
        
        with overlap_col3:
            st.metric(
                "Overlap OranÄ±", 
                f"{overlap_ratio:.1f}%",
                help="Tekrar eden cÃ¼mlelerin yÃ¼zdesi"
            )
        
        with overlap_col4:
            st.metric(
                "Ort. KullanÄ±m/CÃ¼mle", 
                f"{avg_usage_per_sentence:.2f}x",
                help="Her cÃ¼mle ortalama kaÃ§ kez kullanÄ±ldÄ±"
            )
        
        # Overlap yorumu
        if overlap_ratio < 20:
            st.success("âœ… DÃ¼ÅŸÃ¼k overlap: CÃ¼mleler Ã§oÄŸunlukla bir kez kullanÄ±lmÄ±ÅŸ, temiz veri!")
        elif overlap_ratio < 50:
            st.info("â„¹ï¸ Orta dÃ¼zey overlap: BazÄ± cÃ¼mleler birden fazla kez kullanÄ±lmÄ±ÅŸ, kabul edilebilir.")
        else:
            st.warning("âš ï¸ YÃ¼ksek overlap: AynÄ± cÃ¼mleler Ã§ok kez kullanÄ±lmÄ±ÅŸ. Ä°statistikler ÅŸiÅŸik olabilir.")
        
        # Dosya bazlÄ± overlap analizi
        with st.expander("ğŸ“‚ Dosya BazlÄ± Overlap DetaylarÄ±"):
            file_overlap_data = []
            
            for fname, sentence_list in file_sentence_usage.items():
                total_usages = len(sentence_list)
                unique_sentences = len(set(sentence_list))
                
                if total_usages > 0:
                    file_overlap = ((total_usages - unique_sentences) / total_usages) * 100
                    avg_usage = total_usages / unique_sentences if unique_sentences > 0 else 0
                else:
                    file_overlap = 0
                    avg_usage = 0
                
                file_overlap_data.append({
                    'Dosya': fname,
                    'Toplam KullanÄ±m': total_usages,
                    'Benzersiz CÃ¼mle': unique_sentences,
                    'Overlap %': f"{file_overlap:.1f}%",
                    'Ort. KullanÄ±m': f"{avg_usage:.2f}x"
                })
            
            file_overlap_df = pd.DataFrame(file_overlap_data)
            file_overlap_df = file_overlap_df.sort_values('Toplam KullanÄ±m', ascending=False)
            st.dataframe(file_overlap_df, use_container_width=True)
        
        st.markdown("---")
        
        # Grafikler
        st.subheader("ğŸ“Š GÃ¶rselleÅŸtirmeler")
        
        # 1. Sentiment DaÄŸÄ±lÄ±mÄ±
        st.markdown("### 1ï¸âƒ£ Sentiment DaÄŸÄ±lÄ±mÄ± (Model 1 & 2)")
        fig_sentiment = create_sentiment_distribution_chart(results)
        if fig_sentiment:
            st.plotly_chart(fig_sentiment, use_container_width=True, key="sentiment_dist_chart")
        
        st.markdown("---")
        
        # 2. Dosya BazlÄ± Analiz
        st.markdown("### 2ï¸âƒ£ Dosya BazlÄ± EÅŸleÅŸme SayÄ±larÄ±")
        fig_files = create_file_summary_chart(results)
        if fig_files:
            st.plotly_chart(fig_files, use_container_width=True, key="file_summary_chart")
        
        # Dosya detay tablosu
        with st.expander("ğŸ“‚ TÃ¼m Dosyalar - DetaylÄ± Tablo"):
            file_summary = {}
            for r in results:
                fname = r.get('filename', 'N/A')
                if fname not in file_summary:
                    file_summary[fname] = {
                        'EÅŸleÅŸme SayÄ±sÄ±': 0,
                        'Positive (M1)': 0,
                        'Negative (M1)': 0,
                        'Neutral (M1)': 0
                    }
                file_summary[fname]['EÅŸleÅŸme SayÄ±sÄ±'] += 1
                sentiment = r.get('model_1', {}).get('sentiment', '').lower()
                if 'positive' in sentiment:
                    file_summary[fname]['Positive (M1)'] += 1
                elif 'negative' in sentiment:
                    file_summary[fname]['Negative (M1)'] += 1
                elif 'neutral' in sentiment:
                    file_summary[fname]['Neutral (M1)'] += 1
            
            file_df = pd.DataFrame.from_dict(file_summary, orient='index')
            file_df = file_df.sort_values('EÅŸleÅŸme SayÄ±sÄ±', ascending=False)
            st.dataframe(file_df, use_container_width=True)
        
        st.markdown("---")
        
        # 3. Anahtar Kelime Analizi
        st.markdown("### 3ï¸âƒ£ Anahtar Kelime BazlÄ± EÅŸleÅŸmeler")
        fig_keywords = create_keyword_summary_chart(results)
        if fig_keywords:
            st.plotly_chart(fig_keywords, use_container_width=True, key="keyword_summary_chart")
        
        # Anahtar kelime detay tablosu
        with st.expander("ğŸ”‘ Anahtar Kelimeler - DetaylÄ± Tablo"):
            keyword_summary = {}
            for r in results:
                kw = r.get('keyword', 'N/A')
                if kw not in keyword_summary:
                    keyword_summary[kw] = {
                        'EÅŸleÅŸme SayÄ±sÄ±': 0,
                        'Positive (M1)': 0,
                        'Negative (M1)': 0,
                        'Neutral (M1)': 0
                    }
                keyword_summary[kw]['EÅŸleÅŸme SayÄ±sÄ±'] += 1
                sentiment = r.get('model_1', {}).get('sentiment', '').lower()
                if 'positive' in sentiment:
                    keyword_summary[kw]['Positive (M1)'] += 1
                elif 'negative' in sentiment:
                    keyword_summary[kw]['Negative (M1)'] += 1
                elif 'neutral' in sentiment:
                    keyword_summary[kw]['Neutral (M1)'] += 1
            
            keyword_df = pd.DataFrame.from_dict(keyword_summary, orient='index')
            keyword_df = keyword_df.sort_values('EÅŸleÅŸme SayÄ±sÄ±', ascending=False)
            st.dataframe(keyword_df, use_container_width=True)
        
        st.markdown("---")
        
        # 4. Model 3 - Top Duygular
        st.markdown("### 4ï¸âƒ£ En SÄ±k GÃ¶rÃ¼len Duygular (Model 3)")
        all_emotions = {}
        for r in results:
            top_emotion = r.get('model_3', {}).get('top_emotions', [{}])[0]
            emotion_label = top_emotion.get('label', 'unknown')
            if emotion_label != 'unknown':
                all_emotions[emotion_label] = all_emotions.get(emotion_label, 0) + 1
        
        if all_emotions:
            emotion_series = pd.Series(all_emotions).sort_values(ascending=False)
            
            import plotly.graph_objects as go
            fig_emotions = go.Figure()
            fig_emotions.add_trace(go.Bar(
                x=emotion_series.index[:15],  # Top 15
                y=emotion_series.values[:15],
                marker_color='purple'
            ))
            fig_emotions.update_layout(
                title='En SÄ±k Tespit Edilen 15 Duygu (Model 3)',
                xaxis_title='Duygu',
                yaxis_title='Frekans',
                height=400
            )
            st.plotly_chart(fig_emotions, use_container_width=True, key="emotion_freq_chart")
        
    else:
        st.info("ğŸ‘ˆ Ã–nce 'Dosya YÃ¼kle' sekmesinden dosya yÃ¼kleyin ve analiz baÅŸlatÄ±n.")

with tab4:
    st.header("â„¹ï¸ Proje HakkÄ±nda")
    
    st.markdown("""
    ### ğŸ¯ AmaÃ§
    Qatar DÃ¼nya KupasÄ± ile ilgili Almanca haber metinlerinde duygu analizi yapmak ve 
    farklÄ± modellerin performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak.
    
    ### ğŸ“š KullanÄ±lan Modeller
    
    **1. Model 1: HÄ±zlÄ± Pilot**
    - Oliver Guhr et al. (LREC 2020)
    - 1.8M Almanca Ã¶rnek
    - 3 kategori: positive, negative, neutral
    - F1 Score: ~0.96
    
    **2. Model 2: Haber Metinleri**
    - mdraw/german-news-sentiment-bert
    - Haber dili iÃ§in Ã¶zelleÅŸtirilmiÅŸ
    - 2007-2019 gÃ¶Ã§ haberleri ile eÄŸitilmiÅŸ
    
    **3. Model 3: DetaylÄ± Akademik**
    - GoEmotions (Google, ACL 2020)
    - 27 farklÄ± duygu kategorisi
    - Ã‡ok dilli BERT
    
    ### ğŸ”¬ Metodoloji
    - **Context Window:** Anahtar kelimenin 3 cÃ¼mle Ã¶ncesi ve sonrasÄ±
    - **Ã‡oklu Model:** ÃœÃ§ farklÄ± yaklaÅŸÄ±mÄ±n karÅŸÄ±laÅŸtÄ±rmasÄ±
    - **Akademik Standart:** Peer-reviewed modeller
    - **Overlap Stratejisi:** TÃ¼m eÅŸleÅŸmeler analiz edilir (maksimum kapsam)
    
    ### ğŸ“– Kaynaklar
    - [Guhr et al. 2020 - LREC](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)
    - [GoEmotions - ACL 2020](https://aclanthology.org/2020.acl-main.372/)
    - [GitHub Repository](https://github.com/laikakos/laikaresearch)
    
    ### âš¡ Performans Ä°puÃ§larÄ± (2500 PDF iÃ§in)
    - Dosyalar batch olarak iÅŸlenir
    - Her dosya iÃ§in canlÄ± ilerleme takibi
    - HatalÄ± dosyalar atlanÄ±r ve listelenir
    - Toplam sÃ¼re: ~30-90 dakika (dosya boyutuna baÄŸlÄ±)
    - Overlap analizi ile veri kalitesi takibi
    
    ### ğŸ“Š Ã–zellikler
    - **CanlÄ± Ä°lerleme:** Dosya baÅŸÄ±, eÅŸleÅŸme sayÄ±sÄ±, tahmini kalan sÃ¼re
    - **Overlap Analizi:** AynÄ± cÃ¼mlelerin kaÃ§ kez kullanÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir
    - **Ä°statistikler Sekmesi:** Genel gÃ¶rselleÅŸtirmeler ve trendler
    - **Dosya BazlÄ± Analiz:** Her dosyanÄ±n detaylÄ± sentiment daÄŸÄ±lÄ±mÄ±
    - **Anahtar Kelime Analizi:** Hangi kelime ne kadar etkili
    - **Model 3 Duygu HaritasÄ±:** En sÄ±k gÃ¶rÃ¼len 15 duygu
    """)

st.markdown("---")
st.markdown("ğŸ’¡ **Ä°pucu:** Sidebar'dan anahtar kelimeleri ve context window ayarlarÄ±nÄ± Ã¶zelleÅŸtirebilirsiniz.")
